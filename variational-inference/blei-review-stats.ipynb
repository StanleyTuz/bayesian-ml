{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KL divergence between the posterior distribution $p\\left(z\\middle|x\\right)$, which we want to know, and the approximation to the posterior, $q\\left(z\\right)$, is defined as $$ \\text{KL}\\left( q\\left(z\\right) \\left\\lVert \\, p\\left(z \\middle| x\\right) \\right. \\right) = \\mathbb{E}_q \\left[ \\log q\\left(z\\right)\\right] - \\mathbb{E}_q \\left[ \\log p\\left(z\\middle|x\\right)\\right]  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting out, we don't actually know the posterior, so we want to rewrite this without that distribution. Recall that\n",
    "\n",
    "$$ p\\left(z\\middle|x\\right) = \\frac{p\\left(z,x\\right)}{p\\left(x\\right)}$$\n",
    "\n",
    "so that we have \n",
    "\n",
    "\\begin{align} \\text{KL}\\left( q\\left(z\\right) \\left\\lVert \\, p\\left(z \\middle| x\\right) \\right. \\right) & = \\mathbb{E}_q \\left[ \\log q\\left(z\\right)\\right] - \\mathbb{E}_q \\left[ \\log p\\left(z,x\\right)\\right] + \\mathbb{E}_q \\left[ \\log p\\left(x\\right) \\right] \\\\ & = \\mathbb{E}_q \\left[ \\log q\\left(z\\right)\\right] - \\mathbb{E}_q \\left[ \\log p\\left(z,x\\right)\\right] + \\log p\\left(x\\right)\n",
    " \\end{align}\n",
    "\n",
    "where the expectation in the last term is trivial, since the argument doesn't depend on $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rightmost term involves the evidence, $p\\left(x\\right)$, which is generally difficult or impossible to calculate (that's what makes Bayesian inference hard!). However, from the point of view of an approximation $q\\left(z\\right)$, this term is a constant. We will focus instead on the quantity\n",
    "\n",
    "\\begin{align} \\text{ELBO}\\left(q\\right) & = - \\text{KL}\\left( q\\left(z\\right) \\left\\lVert \\, p\\left(z \\middle| x\\right) \\right. \\right) +  \\mathbb{E}_q \\left[ \\log p\\left(x\\right) \\right] \\\\ & = \\mathbb{E}_q \\left[ \\log p\\left(z, x\\right)\\right] - \\mathbb{E}_q \\left[ \\log q\\left(z\\right)\\right] \\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we factor the joint distribution in the first term in the other direction --- $p\\left(z,x\\right) = p\\left(z\\right)p\\left(x\\middle|z\\right)$ --- then we can see that due to the non-negativity of the KL divergence the ELBO bounds the evidence below, hence the name Evidence Lower BOund."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ELBO is the negative of the KL divergence plus a constant, minimizing the KL divergence is equivalent to maximizing the ELBO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal**: given some parametric family $\\mathcal{D}$ of approximate distributions $q$, find the best approximation $q\\left(z\\right)$ to the posterior $p\\left(z\\middle| x\\right)$ in the sense that the ELBO is maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean-field family"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean-field variational family is one such family $\\mathcal{D}$ of approximations $q$. They assume independence between the latent variables, so that $$ q\\left(z\\right) = \\prod_{j=1}^m q_j\\left(z_j\\right)$$ where each $q_j$ is a distribution with its own parameters.\n",
    "\n",
    "This family is conceptually simple, but does not capture any correlation between variables, which can be a pretty big limitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 1: CAVI\n",
    "\n",
    "A particularly simple algorithm for performing the optimization is to just do ascent on each latent variable coordinate in sequence. \n",
    "\n",
    "For example, consider the mean-field family, above, for a problem with $m$ latent variables. First, we will treat variables $z_2,z_3,\\ldots,z_m$ and their associated mean-field factors $q_2,q_3,\\ldots,q_m$ as fixed and consider optimizing $\\text{ELBO}\\left(q\\right)$ with respect to only the first factor: $\\text{ELBO}\\left(q\\right) = \\text{ELBO}\\left(q_1\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we write this out for arbitrary factor $j$, then we have\n",
    "\n",
    "\\begin{align} \\text{ELBO}\\left(q_j\\right) & = \\mathbb{E}_j \\left[ \\mathbb{E}_{-j} \\left[ \\log p\\left(z_j, z_{-j}, x\\right)\\right]\\right] - \\mathbb{E}_j\\left[ \\log q_j\\left(z_j\\right) \\right] + \\text{const.} \\\\ & = - \\text{KL}\\left(\\log q_j\\left(z_j\\right) \\lVert \\, \\mathbb{E}_{-j} \\left[ \\log p\\left(z_j, z_{-j}, x\\right)\\right] \\right) + \\text{const.}  \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we have written the ELBO (as a function of only one factor) as a negative KL divergence between that factor and another distribution, plus an additive constant. Again, to maximize the ELBO we can minimize the negative KL divergence, which in this case will be minimized when we take $$ \\log q_j\\left(z_j\\right) = \\mathbb{E}_{-j} \\left[ \\log p\\left(z_j, z_{-j}, x\\right) \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside \n",
    "\n",
    "Note that the KL divergence is computed between two distributions, *not* between two scalars. When writing the KLD between two distributions, we have written out the random variables as \"arguments\" to the distributions, but this was misleading. It should really have been written $$ \\text{KL}\\left(q \\left\\lVert  \\, p \\right. \\right)$$ but this is misleading for other reasons: we have multiple distributions \"named\" $p$. The KLD is defined in terms of expected values of distributions, and so what comes out of it should be a number.\n",
    "\n",
    "Thus, to minimize the ELBO, we set the two function in the KLD equal to each other. This means that for every possible value of $z_j$, we have $$ \\log q_j \\left(z_j\\right) = \\mathbb{E}_{-j}\\left[ \\log p\\left(z_j, z_{-j}, x\\right)\\right] $$ or $$ q_j\\left(z_j\\right) = \\exp \\left\\{ \\mathbb{E}_{-j}\\left[ \\log p\\left(z_j, z_{-j}, x\\right)\\right] \\right\\} $$ where the expectations are taken over the fixed variational density $$ \\prod_{l \\neq j} q_l\\left(z_l\\right)$$ over $z_{-j}$. Finally, we can once again apply the definition of conditional probability to get $$ q_j \\left(z_j\\right) \\propto \\exp\\left\\{\\mathbb{E}_{-j}\\left[ \\log p\\left(z_j\\, \\middle| \\, z_{-j}, x\\right)\\right] \\right\\}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution on the right is called the *complete conditional of $z_j$*. It is the conditional distribution of $z_j$ given all other variables in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this all mean? It means that if we consider all but one variational factor fixed, we can optimize the ELBO with respect to that unfixed factor by setting it equal to (or proportional to) that function of $z_j$ defined on the right-hand side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: CAVI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a mixture of two Gaussian distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# instantiate parameters\n",
    "mu = np.array([10.0, 16.0])     # [mu_0, mu_1]\n",
    "sigma2 = 2.0\n",
    "p = 0.35                        # p(c_i = 1)\n",
    "n = 200\n",
    "\n",
    "# generate data\n",
    "c_i = ss.bernoulli(p=p).rvs(size=n)\n",
    "x_i = [ss.norm(loc=mu[c], scale=1.0).rvs() for c in c_i]\n",
    "\n",
    "_, ax = plt.subplots(figsize=(5,2))\n",
    "ax.hist(x_i, bins=30, color='tab:blue', alpha=0.75)\n",
    "ax.set(xlabel='x_i', ylabel='freq', title=\"data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full joint density is\n",
    "\n",
    "\\begin{align*}\n",
    "p\\left(\\mu_{0:1}, c_{1:n}, x_{1:n}\\right) & = p\\left(\\mu_{0:1}\\right) \\prod_{i=1}^n p\\left(c_i\\right) p\\left(x_i \\, \\middle| \\, \\, c_i, \\mu_{0:1}\\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think we will need to calculate, by hand, the complete conditional of each of the latent variables. So what are each of the distributions here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the (full) ELBO for this problem? The variational parameters are $m$, $s^2$, and $\\varphi$. Hence, $q$ is always (at least implicitly) a function of these parameters.\n",
    "\n",
    "The full joint distribution $p\\left(z,x\\right)$ is our model, written above; the variational distribution $q\\left(z\\right)$ is our mean-field distribution over all latent variables in the model: $\\mu_0, \\mu_1$, and $c_i$ for all observations $i=1,2,\\ldots,n$. It looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    q\\left(z\\right) = q\\left(z \\, ; \\, m, s^2, \\varphi \\right) & = q\\left( \\mu_0 \\, ; \\, m_0, s_0^2\\right)q\\left( \\mu_1 \\, ; \\, m_1, s_1^2\\right) \\left( \\prod_{i=1}^n q\\left(c_i \\, ; \\, \\varphi_i \\right) \\right) \\\\\n",
    "    & = q\\left( \\mu_0 \\right)q\\left( \\mu_1 \\right) \\prod_{i=1}^n q\\left(c_i \\right) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The notation here might be confusing. Each factor is labeled \"$q$\", but each represents a different distribution; these are the \"$q_j$\" in the paper. Each distribution is over an individual latent parameter or variable in the model. They are parametrized by the variational parameters, which we will tune to find the \"best\" distribution. To make it more confusing, in the second line I suppressed the distribution parameters for conciseness, but keep in mind that they are really there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The \"full ELBO\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full ELBO can now be easily written down by plugging those two distributions into the formula. The logs in the expectations allow us, as usual, as a series of sums instead of a bunch of products.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{ELBO}\\left(m, s^2, \\varphi\\right) & = \\mathbb{E}_q\\left[\\log p\\left(z,x\\right)\\right] - \\mathbb{E}_q\\left[ \\log q\\left(z\\right) \\right] \\\\\n",
    "& = \\mathbb{E}_q\\left[ \\log p\\left(\\mu_{0:1}\\right) \\prod_{i=1}^n p\\left(c_i\\right) p\\left(x_i \\, \\middle| \\, \\, c_i, \\mu_{0:1}\\right) \\right] \\\\\n",
    "& \\; - \\mathbb{E}_q \\left[ \\log q\\left( \\mu_0 \\right)q\\left( \\mu_1 \\right) \\prod_{i=1}^n q\\left(c_i \\right) \\right]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the expectation of the joint distribution (the model), we end up with terms involving the prior and terms involving the likelihood. As expected, there is a big sum over all of the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\log p\\left(z,x\\right) = \\log p\\left(\\mu_0, \\mu_1, c_{1:n}, x_{1:n}\\right) & = \\log p\\left(\\mu_0\\right) + \\log p\\left(\\mu_1\\right) \\\\\n",
    "    & + \\sum_{i=1}^n \\left( \\log p\\left(c_i\\right) + \\log p\\left(x_i \\, \\middle| \\, \\, c_i, \\mu_0, \\mu_1\\right) \\right)\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the expectation of the variational approximation $q$, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    \\log q\\left(z\\right) = \\log q\\left(\\mu_0, \\mu_1, c_{1:n}\\right) & = \\log q\\left(\\mu_0, \\mu_1, c_{1:n} \\, ; \\, m_{0:1}, s_{0:1}^2, \\varphi_{1:n} \\right) \\\\\n",
    "    & = \\log q\\left(\\mu_0 \\, ; \\, m_0, s_0^2\\right) + \\log q\\left( \\mu_1\\,;\\, m_1, s_1^2 \\right) \\\\\n",
    "    & + \\sum_{i=1}^n \\log q\\left( c_i \\,;\\, \\varphi_i\\right)\n",
    "\\end{align*}\n",
    "\n",
    "where in the second line we explicitly indicate the dependence of the distributions $q$ on the variational parameters. I'm not sure why there is a $\\varphi_i$ for every observation and not just a global $\\varphi$ parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full ELBO is thus the sum of the expectations of both of these quantities. The expectation is taken with respect to the variational distribution $q$, and thanks to the linearity of the expectation, it distributes over all of the terms.\n",
    "\n",
    "According to the paper,\n",
    ">In each term, we have made explicit the dependence on the variational parameters. Each expectation can be computed in closed form.\n",
    "\n",
    "To actually implement the ELBO loss function, we will need to compute these closed-form representations of the expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    " \\mathbb{E}_q \\left[\\log q\\left(\\mu_k\\,; \\, m_k, s_k^2 \\right)\\right] & =  -\\frac{1}{2}\\log 2\\pi s_k^2 - \\frac{1}{2} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    " \\mathbb{E}_q\\left[\\log q\\left(c_i \\right)\\right] & =  \\int d\\varphi_{ik} \\, \\log \\varphi_{ik} \\, q\\left(c_i\\right) \\\\\n",
    " & =  \\int d\\varphi_{ik} \\, \\varphi_{ik} \\, \\log \\varphi_{ik} \\\\\n",
    " & = 1/4\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbb{E}_q \\left[ \\log p\\left(c_i\\right)\\right] & = \\log 1/K \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_q \\left[ \\log p\\left(\\mu_k\\right)\\right] & = -\\frac{1}{2}\\log 2\\pi \\sigma^2 - \\frac{1}{2\\sigma^2} m_k \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\mathbb{E}_q\\left[ \\log p\\left(x_i \\, \\middle| \\, c_i, \\mu\\right)\\right] & = -\\frac{1}{2}\\log 2\\pi - \\frac{1}{2}x_i^2 + x_i m_k - \\frac{1}{2}\\left(m_k^2 + s_k^2\\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The CAVI update for assignments\n",
    "\n",
    "The CAVI algorithm will update each assignment parameter $\\varphi_i$. In order to apply this, we need an expression for the update step.\n",
    "\n",
    "Recall that the update formula in terms of the complete conditional of $c_i$ looks like $$ q\\left(c_i\\right) \\propto \\exp \\left\\{  \\mathbb{E}_{-j}\\left[ \\log p\\left(c_i \\, \\middle| \\, z_{-j}, x \\right) \\right]\\right\\} $$ Equivalently, we can use the full joint distribution: $$ q\\left(c_i\\right) \\propto \\exp \\left\\{  \\mathbb{E}_{-j}\\left[ \\log p\\left(c_i , z_{-j}, x \\right) \\right]\\right\\} $$\n",
    "\n",
    "so let's calculate this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\log p\\left(c_i, z_{-j}, x\\right) = \\log p\\left(c_i\\right) + \\log p\\left(x_i\\,\\middle|\\,c_i, \\mu_{0:1}\\right) + \\text{const.}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood for the mixture model, in the second term above, looks like $$ p\\left(x_i\\,\\middle|\\,c_i,\\mu_{0:1}\\right) = p\\left(x_i\\middle|\\mu_0\\right)^{c_{i0}} p\\left(x_i\\middle|\\mu_1\\right)^{c_{i1}}$$ where exactly one of $c_{i0}$ and $c_{i1}$ is 1 and the other is 0. Taking the log, $$ \\log p\\left(x_i\\,\\middle|\\,c_i,\\mu_{0:1}\\right) = c_{i0} \\log p\\left(x_i\\middle|\\mu_0\\right) + c_{i1} \\log p\\left(x_i\\middle|\\mu_1\\right) $$\n",
    "\n",
    "The probability factors here come from the unit normal distribution --- this is our model likelihood. Thus, we have \n",
    "\\begin{align*}\n",
    "    \\log p\\left(x_i\\,\\middle|\\,c_i,\\mu_{0:1}\\right) & = - c_{i0} \\frac{\\left(x_i - \\mu_0\\right)^2}{2}  - c_{i1} \\frac{\\left(x_i - \\mu_1\\right)^2}{2} \\\\\n",
    "    & = - \\frac{1}{2}c_{i0}\\mu_0^2 - \\frac{1}{2}c_{i1}\\mu_1^2 + c_{i0}\\mu_0x_i + c_{i1}\\mu_1x_i + \\text{const.}\n",
    "\\end{align*}\n",
    "where the constant term includes everything independent of the variational parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next need to take the expectation with respect to $q_{-j}$, where $-j$ is all latent variables which are not the $i$-th assignment variable $c_i$:\n",
    "\\begin{align*}\n",
    "    \\mathbb{E}_{-j}\\left[\\log p\\left(x_i\\,\\middle|\\,c_i,\\mu_{0:1}\\right)\\right] & = \\int_{z_{-j}} dz_{-j} q_{-j}\\left(z_{-j}\\right) \\log p\\left(x_i\\,\\middle|\\,c_i,\\mu_{0:1}\\right) \\\\\n",
    "    & = -\\frac{1}{2} \\sum_{i=1}^2 c_{ik} \\int_{\\mu_k} d\\mu_k \\,\\mu_k^2 \\, q\\left(\\mu_k\\, ;\\, m_k,s_k^2\\right) + x_ic_{ik}\\sum_{k=1}^2 \\int_{\\mu_k} d\\mu_k \\, \\mu_k \\, q\\left(\\mu_k \\,;\\, m_k, s_k^2\\right) \\\\\n",
    "    & = -\\frac{1}{2}\\sum_{i=1}^2 c_{ik} \\mathbb{E}_{q\\left(\\mu_k\\right)}\\left[ \\mu_k^2 \\right] + x_i c_{ik}\\sum_{k=1}^2 \\mathbb{E}_{q\\left(\\mu_k\\right)}\\left[ \\mu_k \\right]\n",
    "\\end{align*}\n",
    "where we collapsed into the sum notation for conciseness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expectations in the last line are simply expectations taken with respect to the variational factor $q\\left(\\mu_k\\right)$. In this problem, this is taken to be a normal distribution with mean $m_k$ and variance $s_k^2$. Hence we can look up these quantities:\n",
    "\\begin{align*}\n",
    "    \\mathbb{E}_{q\\left(\\mu_k\\right)}\\left[ \\mu_k\\right] & = \\mu_k \\\\\n",
    "    \\mathbb{E}_{q\\left(\\mu_k^2\\right)}\\left[ \\mu_k\\right] & = \\mu_k^2 + s_k^2 \\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the integrals here do not depend at all on the data; they are just evaluations based on the current \"state\" of our approximating distribution $q$. As the paper puts it, the integrals (expectations) are \n",
    ">...both computable from the variational Gaussian on the $k$-th mixture component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the quantity we seek is \n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{-j}\\left[\\log p\\left(x_i\\,\\middle|\\,c_i,\\mu_{0:1}\\right)\\right] & = -\\frac{1}{2}\\sum_{i=1}^2 c_{ik} \\left( \\mu_k^2 + s_k^2 \\right) + x_i c_{ik} \\sum_{k=1}^2 \\mu_k \\\\ \n",
    "& = \\sum_{k=1}^2 c_{ik}\\left[ x_i \\mu_k - \\frac{1}{2}c_{ik}\\left(\\mu_k^2 + s_k^2\\right) \\right]\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put together, the update formula for the $i$-th observation's cluster assignment is \n",
    "$$ \\varphi_{ik} \\propto \\exp\\left\\{  x_i \\mu_k - \\frac{1}{2}\\left(\\mu_k^2 + s_k^2\\right) \\right\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still not sure exactly what the difference between the $c_{ik}$ and the $\\varphi_{ik}$ are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The CAVI update for cluster means\n",
    "\n",
    "We do the same calculation, now for the cluster means. As before, we can use the full joint distribution: $$ q\\left(\\mu_k\\right) \\propto \\exp \\left\\{  \\mathbb{E}_{-j}\\left[ \\log p\\left(\\mu_k , z_{-j}, x \\right) \\right]\\right\\} $$\n",
    "\n",
    "where $z_{-j}$ represents every variable which is not $\\mu_k$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we write this out, retaining only those terms involving $\\mu_k$, we end up with something like $$ \\log q\\left(\\mu_k\\right) = \\left(\\sum_{i=1}^n \\varphi_{ik}x_i\\right) \\mu_k - \\frac{1}{2}\\left(\\sigma^2 + \\sum_{i=1}^n \\varphi_{ik}\\right)\\mu_k^2 + \\text{const.} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is a Gaussian distribution where we can read the parameters off. Hence, the update formulas for the variational parameters are $$ m_k \\leftarrow \\frac{\\sum_{i=1}^n \\varphi_{ik} x_i}{\\sigma^{-2} + \\sum_{i=1}^n \\varphi_{ik}} $$ \n",
    "and \n",
    "$$ s_k^2 \\leftarrow  \\frac{1}{\\sigma^{-2} + \\sum_{i=1}^n \\varphi_{ik}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_idx = np.expand_dims(np.random.randint(0,2, size=(5,)), axis=1) # index\n",
    "a = np.zeros((5, 2), dtype=int)\n",
    "# ai = np.expand_dims(a, axis=1)\n",
    "# # np.expand_dims(a, axis=1)\n",
    "np.put_along_axis(a, a_idx, 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the variational parameters\n",
    "K = 2\n",
    "mk = np.array([10.0, 15.0])\n",
    "sk2 = np.array([1.5, 1.5])\n",
    "\n",
    "# phi_ik = prob of data i coming from cluster k\n",
    "phi_idx = np.expand_dims(np.random.randint(0, 2, size=(len(x_i),)), axis=1) # index\n",
    "phi_ik = np.zeros((len(x_i), K), dtype=int)\n",
    "np.put_along_axis(phi_ik, phi_idx, 1, axis=1)\n",
    "del phi_idx\n",
    "\n",
    "def calc_elbo_mixture(\n",
    "    x_i: np.ndarray,\n",
    "    mk: np.ndarray,\n",
    "    sk2: np.ndarray,\n",
    "    phi_ik: np.ndarray,\n",
    "    ):\n",
    "    \"\"\"Given the current value of the variational parameters, calculate\n",
    "    the full ELBO (the loss function).\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def cavi_mixture_loop(\n",
    "    x_i: np.ndarray,\n",
    "    mk: np.ndarray,\n",
    "    sk2: np.ndarray,\n",
    "    phi_ik: np.ndarray,\n",
    "    K: int = 2,\n",
    "    n_iterations: int = 20,\n",
    "    ):\n",
    "\n",
    "    elbo_trace = np.zeros(n_iterations+1)\n",
    "    elbo_trace[0] = calc_elbo_mixture(x_i, mk, sk2, phi_ik)\n",
    "\n",
    "    for n_iter in range(n_iterations):\n",
    "        # update assignments, phi\n",
    "        phi_ik = _update_assignments(x_i, mk, sk2)\n",
    "\n",
    "        # update clusters\n",
    "        for k in range(K):\n",
    "            # update mk\n",
    "            mk = _update_cluster_means(x_i, phi_ik, sigma2)\n",
    "            # update sk2\n",
    "            sk2 = _update_cluster_vars(phi_ik)\n",
    "\n",
    "        # compute elbo\n",
    "        elbo_trace[n_iter+1] = calc_elbo_mixture(x_i, mk, sk2, phi_ik)\n",
    "    \n",
    "    return (mk, sk2, phi_ik), elbo_trace\n",
    "\n",
    "\n",
    "def _update_assignments(x_i, mk, sk2):\n",
    "    # \"\"\"Return the updated phi_i matrix.\"\"\"\n",
    "    phi = np.exp(x_i * mk - 0.5 * (mk**2 + sk2))    # (n,K)-matrix\n",
    "    return phi / np.expand_dims(phi.sum(axis=1), axis=1)\n",
    "\n",
    "def _update_cluster_means(x_i, phi_ik, sigma2):\n",
    "    denom = 1/sigma2 + phi_ik.sum(axis=0)   # K-vector\n",
    "    numer = (phi_ik * x_i).sum(axis=0)      # K-vector\n",
    "    return numer / denom\n",
    "\n",
    "def _update_cluster_vars(phi_ik):\n",
    "    denom = 1 / sigma2  + phi_ik.sum(axis=0)    # K-vector\n",
    "    return 1 / denom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_elbo_mixture(\n",
    "    x_i: np.ndarray,\n",
    "    mk: np.ndarray,\n",
    "    sk2: np.ndarray,\n",
    "    phi_ik: np.ndarray,\n",
    "    ):\n",
    "    \"\"\"Given the current value of the variational parameters, calculate\n",
    "    the full ELBO (the loss function). This is built up term-by-term.\n",
    "    \"\"\"\n",
    "    elbo = 0.0\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_i = np.expand_dims(np.array([1,2,3,4]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = x_i * mk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4, 0.6],\n",
       "       [0.4, 0.6],\n",
       "       [0.4, 0.6],\n",
       "       [0.4, 0.6]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi / np.expand_dims(phi.sum(axis=1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10., 15.],\n",
       "       [20., 30.],\n",
       "       [30., 45.],\n",
       "       [40., 60.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('pymc3_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cc620a1e663e2469989c8a420117e86934ce483c1158acdb5849407470120c90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
