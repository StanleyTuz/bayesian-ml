{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A general directed latent-variable model has the form (joint distribution) $$ p\\left(x,z\\right) = p\\left(x\\, \\middle| \\, z\\right)p\\left(z\\right) $$ where $x$ are observed variables and $z\\in \\mathbb{R}^k$ are latent variables.\n",
    "\n",
    "Latent variables are unobserved but may explain features observed in $x$. For instance, if $x$ is the number of visitors to a particular park on a particular day, then we may take $z$ to encode the daily weather.\n",
    "\n",
    "This can be generalized to multiple layers: $$ p\\left(x,z_1,z_2,\\ldots,z_m\\right) = p\\left(x\\,\\middle|\\, z_1\\right) p\\left(z_1 \\,\\middle|\\, z_2\\right)\\cdots p\\left(z_{m-1}\\,\\middle|\\,z_{m}\\right)p\\left(z_m\\right) $$ where the joint factors this way because of the conditional independence of the various latent variables. These multi-layer models capture hierarchies of latent variables and are called *deep generative models*.\n",
    "\n",
    "\n",
    "### how to learn\n",
    "\n",
    "If we have some dataset $D$, how can we leaarn $p$? Generally this involves learning the parameters $\\theta$ of some parameteric representation of $p$.\n",
    "\n",
    "Other tasks we are interested in include (approximate) posterior inference over $z$, i.e., the plausibilities of various $z$ given some data point $x$: $$ p\\left(z\\,\\middle|\\,x\\right) $$ In a lot of situations, this posterior is intractible.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-encoding Variational Bayes\n",
    "\n",
    "AEVB is based on variational inference. This is one algorithm which solves the posterior inference task. It involves\n",
    "1. A reformulation of ELBO into an auto-encoding form\n",
    "2. A black-box variational inference approach\n",
    "3. a reparametrization-based low-variance gradient estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ELBO is $$ \\mathcal{L} \\left(p_{\\theta}, q_{\\phi}\\right) = \\mathbb{E}_{q_{\\phi}\\left(z\\,\\middle|\\,x\\right)} \\left[ \\log p_{\\theta}\\left(x,z\\right) - \\log q_{\\phi}\\left(z\\,\\middle|\\,x\\right) \\right] $$ We want to maximize this over some family of $q_{\\phi}$. Recall that the ELBO is defined so that $$ \\mathcal{L} \\left(p_{\\theta}, q_{\\phi}\\right) + \\text{KL}\\left(q_{\\phi}\\left(z\\,\\middle|\\,x\\right) \\, \\middle\\| \\, p\\left(z\\, \\middle| \\, x\\right) \\right) = \\log p_{\\theta}\\left(x\\right) = \\text{const.} $$ For every $x$, we will obtain a different $q\\left(z\\right)$.\n",
    "\n",
    "At this point, the standard VI approach is to assume that $q_{\\phi}$ comes from the mean-field family, i.e., that $q$ factors over the latent variables. The coordinate ascent algorithm is usually applied to maximize the ELBO. This restriction to the mean-field distributions may be too much.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}