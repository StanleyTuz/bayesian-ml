{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes from the Bishop book on pattern recognition.\n",
    "\n",
    "**Probabilistic graphical models** aid in visualization of complex probabilistic relationships between components of a model.\n",
    "\n",
    "A node is a random variable, and the edges are relationships between these variables. \n",
    "> The graph then captures the way in which the joint distribution over all of the random variables can be decomposed into a product of factors each depending only on a subset of the variables.\n",
    "\n",
    "Directed graphs in particular are useful for showing causal relationships among variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the case of three random variables $a$, $b$, and $c$. We want to look at the joint distribution of these three variables, namely $p\\left(a,b,c\\right)$. This can be factored by using the definition of conditional probability: $$ p\\left(a,b,c\\right) = p\\left(c\\,\\middle|\\,a,b\\right)p\\left(a,b\\right) = p\\left(c\\,\\middle|\\,a,b\\right)p\\left(b\\, \\middle| \\, a\\right)p\\left(a\\right) $$ This joint distribution is represented in graphical form by letting each variable have a node. For each variable, an arrow is drawn pointed to that node corresponding to the variables upon which it is conditioned.\n",
    "\n",
    "Note that we could have chosen any factorization of the joint distribution, and we would have obtained a different graph.\n",
    "\n",
    "Another example would be the joint distribution $$ p\\left(x_1,x_2,\\ldots, x_K\\right) = p\\left(x_K \\, \\middle| \\, x_1,\\ldots, x_{K-1}\\right) \\cdots p\\left(x_2\\,\\middle|\\,x_1\\right)p\\left(x_1\\right) $$\n",
    "\n",
    "Try drawing some graphs and writing the corresponding joint factorization, and vice versa. The interesting part of this is when edges are absent, i.e., when some variables do not depend on others.\n",
    "\n",
    "An important restriction is that these graphs must be directed acyclic graphs (DAGs), since we can't loop back around. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Bayesian polynomial fitting\n",
    "\n",
    "In a typical curve fitting problem, we have training data $\\left(\\vec{x},\\vec{t}\\right)$ and we want to be able to predict the value of $t$ for some new input $x$. This amounts to evaluating the predictive distribution $$ p\\left(t\\, \\middle| \\, x,\\vec{x},\\vec{t}\\right) $$ The treatment becomes Bayesian when we invoke familiar rules of probability to factor this posterior, using the polynomial parameters $\\vec{w}$: $$ p\\left(t\\, \\middle|\\, x, \\vec{x}, \\vec{t}\\right) = \\int p\\left(t\\,\\middle| \\, x,\\vec{w}\\right) p\\left(\\vec{w}\\, \\middle| \\, \\vec{x},\\vec{t}\\right) \\, d\\vec{w} $$ where we assume $$p\\left(t\\, \\middle| \\, x,\\vec{w}\\right) = \\mathcal{N}\\left(t \\, \\middle|\\, y\\left(x,\\vec{w}\\right), \\beta^{-1}\\right) $$ Now we have $p\\left(\\vec{w} \\, \\middle| \\, \\vec{x},\\vec{t}\\right) $ is the posterior distribution over parameters. In this case, where everything is Gaussian, we can do this analytically.\n",
    "\n",
    "In this model, the random variables are the polynomial coefficients $\\vec{w}$ and the observed data $\\vec{t}$. We also have the input data $\\vec{x}$, the noise variance $\\sigma^2$, and the hyperparameter $\\alpha$. These are treated merely as parameters of the model and not as random variables (Note: this is where I'm a little confused, but oh well.). So what is the joint distribution? $$ p\\left(\\vec{t},\\vec{w}\\right) = \\prod_{i=1}^N p\\left(\\vec{w}\\right)p\\left(t_n\\, \\middle| \\, \\vec{w}\\right) = p\\left(\\vec{w}\\right)\\prod_{i=1}^N p\\left(t_n\\, \\middle| \\, \\vec{w}\\right)$$ Why does this make sense? Each observation $t_i$ is a random variable and relies on the polynomial coefficients (in our model). \n",
    "\n",
    "Rather than draw a single node for each observation $t_i$, $1\\leq i \\leq N$, we draw a box called a *plate* around a single node, and label it with $N$ to indicate $N$-times repetition of this node.\n",
    "\n",
    "\n",
    "What if we want to make parameters of a model and stochastic variables *both* explicit in the model? The joint becomes $$ p\\left(\\vec{t}, \\vec{w} \\, \\middle| \\, \\vec{x}, \\alpha, \\sigma^2\\right) = p\\left(\\vec{w}\\, \\middle| \\, \\alpha\\right) \\prod_{i=1}^N p\\left(t_n\\, \\middle| \\, \\vec{w}, x_n, \\sigma^2\\right) $$ One convention for representing deterministic parameters in a graphical model is by small closed circles, rather than the large open circles which indicate stochastic variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In typical machine learning applications, some of the random variables are related to observed values. In this example, the observations $t_n$ are observed, and are called **observed variables**. On the other hand, variables which we do not know by observation are called **latent variables**. In these cases, the latent variables are indicated with a shaded circle, and the latent variables with an unshaded circle. \n",
    "\n",
    "In these models, we can use Bayes' theorem to make inference on the latent variables, i.e., find the posterior of the latent variables conditioned on the rest. We can also seek the distribution of a new observation based on the given data and a novel input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}