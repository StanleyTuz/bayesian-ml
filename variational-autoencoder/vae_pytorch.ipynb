{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELBO Loss: $$ \\min \\mathbb{E}_q \\left[ \\log q\\left(z\\middle|x\\right) - \\log p\\left(z\\right) \\right] - \\mathbb{E}_q \\log p\\left(x\\middle|z\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first expectation is the KL divergence.\n",
    "\n",
    "Think: the first term wants to minimize the (expected) difference between the probs given by $p$ and $q$. We're controlling $q$ here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.2009)\n"
     ]
    }
   ],
   "source": [
    "p = torch.distributions.Normal(loc=0, scale=1)\n",
    "q = torch.distributions.Normal(loc=2, scale=4)\n",
    "\n",
    "# take a sample\n",
    "z = q.rsample()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a sample from $q$, we can take the logs in the KL divergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob pz:  0.0024\n",
      "prob qzx: 0.0953\n",
      "kld (single point): 3.6914\n"
     ]
    }
   ],
   "source": [
    "log_pz = p.log_prob(z)\n",
    "log_qzx = q.log_prob(z)\n",
    "\n",
    "print(f\"prob pz:  {torch.exp(log_pz):.4f}\")\n",
    "print(f\"prob qzx: {torch.exp(log_qzx):.4f}\")\n",
    "\n",
    "kld = log_qzx - log_pz\n",
    "print(f\"kld (single point): {kld:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the probs of observing $z$ from the two distributions. If these two probs are vastly different for a given $x$, then if we do this over all $x$ and take the expected value, this difference will be large. Thus the KL divergence will be large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can move the distributions closer together and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kld: 7.2369\n"
     ]
    }
   ],
   "source": [
    "p = torch.distributions.Normal(loc=0, scale=1)\n",
    "q = torch.distributions.Normal(loc=1, scale=2)\n",
    "\n",
    "# plot\n",
    "# ...\n",
    "\n",
    "# take a sample\n",
    "z = q.rsample()\n",
    "kld = q.log_prob(z) - p.log_prob(z)\n",
    "print(f\"kld (single point): {kld:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue: this is technically only difference in the log probabilities for a single point $z \\sim q$; what we *really* want is the **expected difference** under all possible values for $z$ (as drawn from $q$)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(z, mu, std):\n",
    "    p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "    q = torch.distributions.Normal(mu, std)\n",
    "\n",
    "    log_qzx = q.log_prob(z)\n",
    "    log_pz = p.log_prob(z)\n",
    "\n",
    "    kld = log_qzx - log_pz\n",
    "\n",
    "    return kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is $X = \\left\\{ x^{\\left(i\\right)} \\right\\}_{i=1}^N$, where $x$ is discrete or continuous.\n",
    "\n",
    "Assume there is a continuous unobserved/latent rv $z$.\n",
    "\n",
    "The data-generating process is\n",
    "1. $z^{\\left(i\\right)} \\sim p_{\\theta^*}\\left(z\\right)$ (prior)\n",
    "2. $x^{\\left(i\\right)} \\sim p_{\\theta^*}\\left(x \\, \\middle| \\, z^{\\left(i\\right)}\\right)$ (conditional dist / likelihood)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributions $p_{\\theta}\\left(z\\right)$ and $p_{\\theta}\\left(x \\, \\middle| z\\right)$ are assumed to come from parametric familias of distributions. Their PDFs are differentiable a.e. w.r.t. $\\theta$ and $z$ (since $z$ continuous, this is sensible).\n",
    "\n",
    "Q: Why do they use the same variables $p$ and $\\theta$? Seems confusing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we want to do? Efficient and approximate:\n",
    "* ML/MAP estimation of parameters $\\theta$.\n",
    "* posterior inference of latent variable $z$ given observed $x$, given choice of $\\theta$.\n",
    "* marginal inference of variable $x$ (gives us a \"future\" prior over $x$, to be used elsewhere)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define $q_{\\phi}\\left(z \\, \\middle| \\, x\\right)$ to be \"recognition model\" which approximates the true posterior $p_{\\theta}\\left(z\\,\\middle|\\,x\\right)$. This is another parametric distribution, but we keep it very general here (rather than restrict it to be of a certain form).\n",
    "\n",
    "The authors make the analogy of $z$ being like a latent \"code\" for $x$. This mapping between $x$ and $z$ is not one-to-one; rather, it is not known to us, so we can only speak of the mapping in terms of probability.\n",
    "\n",
    "The recognition model $q_{\\phi}\\left(z\\,\\middle|\\,x\\right)$ is a *probabilistic encoder* since it maps an observation $x$ to a distribution of plausible values of $z$ from which $x$ could have been generated.\n",
    "\n",
    "The model $p_{\\theta}\\left(x\\,\\middle| \\, z\\right)$ is a *probabilistic decoder* since it takes a \"code\" $z$ and produces a distribution over possible values of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas:\n",
    "- Write code to compute KLD between two easy dists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The variational bound\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The marginal data likelihood is:\n",
    "$$ \\log p_{\\theta}\\left(\\mathcal{D}\\right) = \\log p_{\\theta}\\left(x^{\\left(i\\right)}, \\ldots, x^{\\left(N\\right)}\\right) = \\sum_{i=1} ^ N \\log p_{\\theta} \\left(x^{\\left(i\\right)}\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write each term in the sum as \n",
    "\n",
    "$$ \\log p_{\\theta}\\left(x^{\\left(i\\right)}\\right) = D_{KL}\\left(q_{\\phi}\\left(z\\, \\middle| \\, x^{\\left(i\\right)}\\right) \\, || \\, p_{\\theta}\\left(z\\, \\middle| \\, x^{\\left(i\\right)}\\right) \\right) + \\mathcal{L}\\left(\\theta, \\, \\phi; \\, x^{\\left(i\\right)}\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first term is the KL divergence of the approximation $q_{\\phi}$ from the true posterior $p_{\\theta}$. This is non-negative, so the sum of the two terms is always greater than the second term. Hence, the second term is called the variational lower bound on the marginal likelihood of datapoint $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By simply rearranging the terms, we have\n",
    "\\begin{align} \n",
    "    \\log p_{\\theta}\\left(x^{\\left(i\\right)}\\right) & \\geq \\mathcal{L}\\left(\\theta, \\phi;\\, x^{\\left\n",
    "    (i\\right)}\\right) \\\\\n",
    "\\end{align}\n",
    "and the variational lower bound is\n",
    "\\begin{align}\n",
    "     \\mathcal{L}\\left(\\theta, \\phi;\\, x^{\\left\n",
    "    (i\\right)}\\right) & = -D_{KL}\\left(q_{\\phi}\\left(z\\,\\middle|\\,x^{\\left(i\\right)}\\right) \\, || \\, p_{\\theta}\\left(z\\right)\\right) + \\mathbb{E}_{q_{\\phi}\\left(z \\, \\middle| \\, x^{\\left(i\\right)}\\right)} \\left[ \\log p_{\\theta} \\left(x^{\\left(i\\right)}\\, \\middle| \\, z\\right) \\right] \\\\\n",
    "\\end{align}\n",
    "\n",
    "Q: where does that expectation come from?\n",
    "\n",
    "It can also be written $$ \\mathcal{L}\\left(\\theta,\\phi; \\, x^{\\left(i\\right)}\\right) = \\mathbb{E}_{q_{\\phi}\\left(z\\middle|x\\right)}\\left[ -\\log q_{\\phi}\\left(z\\middle|x\\right) + \\log p_{\\theta}\\left(x,z\\right) \\right] $$\n",
    "\n",
    "Q: think about and explain this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we choose the parameters $\\phi$ and $\\theta$ so as to maximize $\\mathcal{L}$, then we have maximized a lower bound on the marginal data likelihood; this is making the data likehood as large as possible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be good to be able to take gradients of $L$ with respect to the parameters. But taking it w.r.t. $\\phi$ is an issue. Why?\n",
    "\n",
    "The \"usual (naive) Monte Carlo gradient estimator\" is \n",
    "\\begin{align}\n",
    "    \\nabla_{\\phi} \\mathbb{E}_{q_{\\phi}\\left(z\\right)} \\left[ f\\left(z\\right) \\right] & = \\mathbb{E}_{q_{\\phi}\\left(z\\right)} \\left[ f\\left(z\\right) \\nabla_{q_{\\phi}\\left(z\\right)} \\log q_{\\phi} \\left(z\\right) \\right] \\\\\n",
    "    & \\approx \\frac{1}{L} \\sum_{l=1}^L f\\left(z\\right) \\nabla_{q_{\\phi}\\left(z^{\\left(l\\right)}\\right)} \\log q_{\\phi}\\left(z^{\\left(l\\right)}\\right) \\\\\n",
    "\\end{align}\n",
    "\n",
    "where $$ z^{\\left(l\\right)} \\sim q_{\\phi}\\left(z\\,\\middle|\\,x^{\\left(i\\right)}\\right) $$ is drawn from our approximate posterior.\n",
    "\n",
    "Q: what does this mean? Where does it come from? Should the first $z$ in the sum be $z^{\\left(l\\right)}$?\n",
    "\n",
    "A: It's saying we can approximate the gradient of an expectation of $f\\left(z\\right)$ under a dist over $z$ by taking a bunch ($L$) of samples from the approximate posterior and computing this sum. The sum looks like an expectation value of $f\\left(z\\right)$, but instead of being weighted by the probability distribution, it's weighted by a gradient of the log-distribution. Need to think more about this.\n",
    "\n",
    "The authors claim that this is a high-variance estimator, and so is bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. The SGVB estimator; the AEVB algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a better estimator of the lower bound and its gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given approximate posterior $q_{\\phi}\\left(z\\middle|x\\right)$, we want to reparam the variable $\\tilde{z} \\sim q_{\\phi}\\left(z\\middle|x\\right)$. Let $g_{\\phi}\\left(\\epsilon, x\\right)$ be a differentiable transformation and $\\epsilon$ be a noise variable: $$ \\tilde{z} = g_{\\phi}\\left(\\epsilon, x\\right), \\; x\\sim p\\left(\\epsilon\\right). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing $g$ and $p\\left(\\epsilon\\right)$ is deferred until the next section!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these in hand, we can compute Monte Carlo estimates of expectations:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbb{E}_{q_{\\phi}\\left(z\\middle|x^{\\left(i\\right)}\\right)} \\left[ f\\left(z\\right) \\right] & = \\mathbb{E}_{p\\left(\\epsilon\\right)} \\left[ f\\left(g_{\\phi}\\left(\\epsilon, x\\right)\\right) \\right] \\\\\n",
    "    & \\approx \\frac{1}{L} \\sum_{l=1}^L f\\left(g_{\\phi}\\left(\\epsilon^{\\left(l\\right)}, x^{\\left(i\\right)}\\right)\\right), \\; e^{\\left(l\\right)} \\sim p\\left(\\epsilon\\right) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the variational lower bound is written as an expectation: \n",
    "$$ \\mathcal{L}\\left(\\theta,\\phi; \\, x^{\\left(i\\right)}\\right) = \\mathbb{E}_{q_{\\phi}\\left(z\\middle|x\\right)}\\left[ -\\log q_{\\phi}\\left(z\\middle|x\\right) + \\log p_{\\theta}\\left(x,z\\right) \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can use this Monte Carlo gradient estimator:\n",
    "\n",
    "\\begin{align}\n",
    "    \\tilde{\\mathcal{L}}^A \\left(\\theta, \\phi; \\, x^{\\left(i\\right)}\\right) & = \\frac{1}{L}\\sum_{l=1}^L \\left[ \\log p_{\\theta} \\left(x^{\\left(i\\right)}, z^{\\left(i,l\\right)}\\right) - \\log q_{\\phi}\\left(z^{\\left(i,l\\right)} \\, \\middle| \\, x^{\\left(i\\right)}\\right) \\right] \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we wrote $$ z^{\\left(i,l\\right)} = g_{\\phi}\\left(\\epsilon^{\\left(i,l\\right)}, x^{\\left(i\\right)}\\right) \\text{   and   } \\epsilon^{\\left(l\\right)} \\sim p\\left(\\epsilon\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Reparametrization Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply this trick to a Gaussian distribution\n",
    "$$ z \\sim p\\left(z\\,\\middle|\\,x\\right) = \\mathcal{N}\\left(\\mu,\\sigma^2\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple (and \"valid\") reparametrization is \n",
    "$$ z = g\\left(\\epsilon, x\\right) = \\mu + \\sigma \\epsilon $$\n",
    "where $$ \\epsilon \\sim \\mathcal{N}\\left(0, 1\\right) $$ is an auxiliary noise var. (This is a pretty standard reparametrization; I've seen this before, e.g., in linear regression.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we can compute the expectation of some $f\\left(z\\right)$ w.r.t. this reparametrization as derived in the paper:\n",
    "\\begin{align}\n",
    "    \\mathbb{E}_{q_{\\phi}\\left(z\\middle|x^{\\left(i\\right)}\\right)} \\left[ f\\left(z\\right) \\right] & = \\mathbb{E}_{p\\left(\\epsilon\\right)} \\left[ f\\left(g_{\\phi}\\left(\\epsilon, x\\right)\\right) \\right] \\\\\n",
    "    & \\approx \\frac{1}{L} \\sum_{l=1}^L f\\left(g_{\\phi}\\left(\\epsilon^{\\left(l\\right)}, x^{\\left(i\\right)}\\right)\\right), \\; e^{\\left(l\\right)} \\sim p\\left(\\epsilon\\right) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "    \\mathbb{E}_{p\\left(z\\,\\middle|\\,x\\right)} \\left[ f\\left(z\\right)\\right] & = \\mathbb{E}_{\\mathcal{N}\\left(\\mu,\\,\\sigma^2\\right)} \\left[ f\\left(z\\right)\\right] \\\\\n",
    "    & = \\mathbb{E}_{p\\left(\\epsilon\\right)} \\left[ f\\left( g\\left(\\epsilon, x\\right) \\right)\\right] \\\\\n",
    "    & = \\mathbb{E}_{p\\left(\\epsilon\\right)} \\left[ f\\left( \\mu + \\epsilon \\sigma \\right)\\right] \\\\\n",
    "    & \\approx \\frac{1}{L}\\sum_{l=1}^L f\\left( \\mu + \\epsilon^{\\left(l\\right)}\\sigma \\right), \\; \\epsilon^{\\left(l\\right)} \\sim p\\left(\\epsilon\\right) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Q: where does $x$ go, here?\n",
    "\n",
    "A: $x$ comes up in the distribution we're computing the expectation under. It's a conditioning variable, so it doesn't really play a direct role here. We have defined $p\\left(z\\,\\middle|\\,x\\right) = \\mathcal{N}\\left(\\mu,\\sigma^2\\right)$.\n",
    "\n",
    "Q: How do I interpret this reparametrization?\n",
    "\n",
    "A: We want to compute the expectation of a function under a distribution. This is a normal thing to want to do. The distribution itself may be really complex, though. If we reparametrize the distribution, we can still compute the same expectation as the expectation of a composite function over the reparametrizing distribution. Then, we can approximate the latter expectation as a sum of the function values; this is the **Monte Carlo method** for expectation calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E[f(z)] = 148.063\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "mu = 12.0\n",
    "sigma = 2.0\n",
    "\n",
    "# original distribution\n",
    "p_z_x = torch.distributions.Normal(loc=mu, scale=sigma)\n",
    "\n",
    "# function whose expectation we want\n",
    "f = lambda z: z**2\n",
    "\n",
    "# approximate the expectation of the original function\n",
    "draws_z = p_z_x.sample(torch.Size([10_000]))\n",
    "expect = f(draws_z).sum() / draws_z.shape[0]\n",
    "\n",
    "print(f\"E[f(z)] = {expect.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E[f(mu + eps * sigma)] = 148.517\n"
     ]
    }
   ],
   "source": [
    "# reparametrize\n",
    "p_eps = torch.distributions.Normal(loc=0.0, scale=1.0)\n",
    "\n",
    "draws_eps = p_eps.sample(torch.Size([10_000]))\n",
    "expect = f(mu + sigma * draws_eps).sum() / draws_eps.shape[0]\n",
    "\n",
    "print(f\"E[f(mu + eps * sigma)] = {expect.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(149.6586)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(draws, bins=25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Code!](https://github.com/HIPS/autograd/blob/master/examples/variational_autoencoder.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "05483f9220296ac6701beca760617d71fd368db46c55bd0e8f8bdeca047a6cee"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
