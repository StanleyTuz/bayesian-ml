{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Backpropagating errors to parameters and then updating those parameters by tak- ing the gradient with respect to the loss is the same no matter what the underlying model is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `torch.nn` module contains building blocks for building neural networks. \n",
    "\n",
    "The building blocks are called \"modules\" (analogous to \"layers\" in the literature). The base class is `nn.Module`.\n",
    "\n",
    "A `nn.Module` can have `Parameter` instances as attributes; they can also have `nn.Module` instances as submodules. These must be top-level data attributes! Otherwise use `nn.ModuleList` or `nn.ModuleDict`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One popular examples is `nn.Linear`, an affine transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Module` classes all have `__call__` defined, so we can call them like a function. This computes a forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight',\n",
       "  Parameter containing:\n",
       "  tensor([[0.0589]], requires_grad=True)),\n",
       " ('bias',\n",
       "  Parameter containing:\n",
       "  tensor([0.3777], requires_grad=True))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "linear_model = torch.nn.Linear(in_features=1, out_features=1)\n",
    "\n",
    "list(linear_model.named_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important! `nn.Module` is designed to take multiple samples at once. The zeroth dimension is the batch dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(size=(10, 1))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4366],\n",
       "        [0.4366],\n",
       "        [0.4366],\n",
       "        [0.4366],\n",
       "        [0.4366],\n",
       "        [0.4366],\n",
       "        [0.4366],\n",
       "        [0.4366],\n",
       "        [0.4366],\n",
       "        [0.4366]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 127.5501\n",
      "Epoch 1000, Training loss 3.8621\n",
      "Epoch 2000, Training loss 2.9588\n",
      "Epoch 3000, Training loss 2.9287\n",
      "\n",
      " Parameter containing:\n",
      "tensor([[5.3485]], requires_grad=True) \n",
      " Parameter containing:\n",
      "tensor([-17.1960], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, t_u_train, t_c_train, t_u_val = None, t_c_val = None):\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        t_p_train = model(t_u_train)\n",
    "        loss_train = loss_fn(t_p_train, t_c_train)\n",
    "        # t_p_val = model(t_u_val)\n",
    "        # loss_val = loss_fn(t_p_val, t_c_val)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch == 1 or epoch % 1000 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {loss_train.item():.4f}\")\n",
    "                #   f\" Validation loss {loss_val.item():.4f}\")\n",
    "\n",
    "t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_c = torch.tensor(t_c).unsqueeze(1)\n",
    "t_u = torch.tensor(t_u).unsqueeze(1)\n",
    "t_un = 0.1 * t_u\n",
    "\n",
    "linear_model = torch.nn.Linear(1, 1)\n",
    "optimizer = torch.optim.SGD(\n",
    "    linear_model.parameters(),\n",
    "    lr=1e-2,\n",
    ")\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=3000,\n",
    "    model=linear_model,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    t_u_train=t_un,\n",
    "    t_c_train=t_c,\n",
    ")\n",
    "\n",
    "print(\"\\n\",linear_model.weight,\"\\n\",linear_model.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "05483f9220296ac6701beca760617d71fd368db46c55bd0e8f8bdeca047a6cee"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
