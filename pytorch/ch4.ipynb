{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/deep-learning-with-pytorch/dlwpt-code/tree/master/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 1280, 3)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imageio.v2 as imageio\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "img_arr = imageio.imread(\n",
    "    'https://raw.githubusercontent.com/deep-learning-with-pytorch/dlwpt-code/master/data/p1ch4/image-dog/bobby.jpg'\n",
    "    )\n",
    "img_arr.shape  # (width, height, channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch requires the arrays to be $C\\times H \\times W$. Permuting axes doesn't copy data, but changes the size and stride of the `torch.Tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 720, 1280])\n"
     ]
    }
   ],
   "source": [
    "isinstance(img_arr, np.ndarray)  # True\n",
    "\n",
    "img = torch.from_numpy(img_arr).permute([2, 0, 1])\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a trivial batch of images, containing just our single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "batch = torch.zeros(batch_size, 3, 256, 256, dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_uri_prefix = \"https://raw.githubusercontent.com/deep-learning-with-pytorch/dlwpt-code/master/data/p1ch4/image-cats/\"\n",
    "for idx, filename in enumerate([\"cat1.png\", \"cat2.png\", \"cat3.png\"]):\n",
    "    batch[idx] = torch.from_numpy(imageio.imread(cats_uri_prefix + filename)).permute([2,0,1])[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 256, 256])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch tensors are homogeneous!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "uri = \"https://raw.githubusercontent.com/deep-learning-with-pytorch/dlwpt-code/master/data/p1ch4/tabular-wine/winequality-white.csv\"\n",
    "\n",
    "wineq_numpy = pd.read_csv(uri, delimiter=';')\n",
    "col_list = list(wineq_numpy.columns)\n",
    "wineq_numpy = wineq_numpy.to_numpy(dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4898, 12]), torch.float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wineq = torch.from_numpy(wineq_numpy)\n",
    "wineq.shape, wineq.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = wineq[:, :-1]\n",
    "target = wineq[:, -1].long()  # integer labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encoding of labels\n",
    "# integer encoding induces an ordering on the scores; one-hot does not\n",
    "target_onehot = torch.zeros(target.shape[0], 10)\n",
    "target_onehot.scatter_(1, target.unsqueeze(1), 1.0)  #  in-place\n",
    "# scatter(\n",
    "#   dimension/axis along which the other two args are specified,\n",
    "#   column tensor: indices to scatter (unsqueeze makes it a column tensor)\n",
    "#   elements to scatter or a single scalar to scatter\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does `scatter_` do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's manipulate the rest of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7208e-01, -8.1761e-02,  2.1326e-01,  ..., -1.2468e+00,\n",
       "         -3.4915e-01, -1.3930e+00],\n",
       "        [-6.5743e-01,  2.1587e-01,  4.7997e-02,  ...,  7.3995e-01,\n",
       "          1.3417e-03, -8.2419e-01],\n",
       "        [ 1.4756e+00,  1.7450e-02,  5.4378e-01,  ...,  4.7505e-01,\n",
       "         -4.3677e-01, -3.3663e-01],\n",
       "        ...,\n",
       "        [-4.2043e-01, -3.7940e-01, -1.1915e+00,  ..., -1.3130e+00,\n",
       "         -2.6153e-01, -9.0545e-01],\n",
       "        [-1.6054e+00,  1.1666e-01, -2.8253e-01,  ...,  1.0049e+00,\n",
       "         -9.6251e-01,  1.8574e+00],\n",
       "        [-1.0129e+00, -6.7703e-01,  3.7852e-01,  ...,  4.7505e-01,\n",
       "         -1.4882e+00,  1.0448e+00]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mean = torch.mean(data, dim=0)  # get mean of columns\n",
    "data_var = torch.var(data, dim=0)\n",
    "\n",
    "data_normalized = (data - data_mean) / torch.sqrt(data_var)  # uses broadcasting\n",
    "data_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False,  ..., False, False, False])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target <= 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 3.0000e+00, 1.3000e+01,\n",
       "         1.6000e+01],\n",
       "        [2.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 8.0000e+00, 3.2000e+01,\n",
       "         4.0000e+01],\n",
       "        [3.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 5.0000e+00, 2.7000e+01,\n",
       "         3.2000e+01],\n",
       "        ...,\n",
       "        [1.7377e+04, 3.1000e+01, 1.0000e+00,  ..., 7.0000e+00, 8.3000e+01,\n",
       "         9.0000e+01],\n",
       "        [1.7378e+04, 3.1000e+01, 1.0000e+00,  ..., 1.3000e+01, 4.8000e+01,\n",
       "         6.1000e+01],\n",
       "        [1.7379e+04, 3.1000e+01, 1.0000e+00,  ..., 1.2000e+01, 3.7000e+01,\n",
       "         4.9000e+01]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uri = \"https://raw.githubusercontent.com/deep-learning-with-pytorch/dlwpt-code/master/data/p1ch4/bike-sharing-dataset/hour-fixed.csv\"\n",
    "bikes_numpy = np.loadtxt(\n",
    "    uri,\n",
    "    dtype=np.float32,\n",
    "    delimiter=\",\",\n",
    "    skiprows=1,\n",
    "    converters={1: lambda x: float(x[8:10])})\n",
    "bikes = torch.from_numpy(bikes_numpy)\n",
    "bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "05483f9220296ac6701beca760617d71fd368db46c55bd0e8f8bdeca047a6cee"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
